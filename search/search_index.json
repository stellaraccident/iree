{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IREE \u00b6 * UNDER CONSTRUCTION * Overview \u00b6 Components \u00b6 Frontends \u00b6 Backends \u00b6 Bindings \u00b6","title":"Home"},{"location":"#iree","text":"* UNDER CONSTRUCTION *","title":"IREE"},{"location":"#overview","text":"","title":"Overview"},{"location":"#components","text":"","title":"Components"},{"location":"#frontends","text":"","title":"Frontends"},{"location":"#backends","text":"","title":"Backends"},{"location":"#bindings","text":"","title":"Bindings"},{"location":"backends/cpu-dylib/","text":"Dynamic Library CPU HAL Driver \u00b6 IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc. Get runtime and compiler \u00b6 Get IREE runtime with dylib HAL driver \u00b6 You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for CPU native instructions \u00b6 Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"CPU - Dylib"},{"location":"backends/cpu-dylib/#dynamic-library-cpu-hal-driver","text":"IREE supports efficient model execution on CPU. IREE uses LLVM to compile dense computation in the model into highly optimized CPU native instruction streams, which are embedded in IREE's deployable format as dynamic libraries (dylibs). IREE uses its own low-overhead minimal dynamic library loader to load them and then schedule them with concrete workloads onto various CPU cores. Todo Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.","title":"Dynamic Library CPU HAL Driver"},{"location":"backends/cpu-dylib/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"backends/cpu-dylib/#get-iree-runtime-with-dylib-hal-driver","text":"You will need to get an IREE runtime that supports the dylib HAL driver so it can execute the model on CPU via dynamic libraries containing native CPU instructions.","title":"Get IREE runtime with dylib HAL driver"},{"location":"backends/cpu-dylib/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib HAL driver is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DyLib to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"backends/cpu-dylib/#get-compiler-for-cpu-native-instructions","text":"","title":"Get compiler for CPU native instructions"},{"location":"backends/cpu-dylib/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the LLVM-based dylib compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"backends/cpu-dylib/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The dylib compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add DYLIB-LLVM-AOT to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"backends/cpu-dylib/#compile-and-run-the-model","text":"With the compiler and runtime for dynamic libraries, we can now compile a model and run it on the CPU.","title":"Compile and run the model"},{"location":"backends/cpu-dylib/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"backends/cpu-dylib/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = dylib-llvm-aot \\ iree_input.mlir -o mobilenet-dylib.vmfb Todo Choose the suitable target triple for the current CPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"backends/cpu-dylib/#run-the-model","text":"","title":"Run the model"},{"location":"backends/cpu-dylib/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = dylib \\ --module_file = mobilenet-dylib.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"backends/gpu-vulkan/","text":"Vulkan GPU HAL Driver \u00b6 IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc. Prerequisites \u00b6 In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Get runtime and compiler \u00b6 Get IREE runtime with Vulkan HAL driver \u00b6 Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan. Build runtime from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target). Get compiler for SPIR-V exchange format \u00b6 Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into. Download as Python package \u00b6 Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command. Build compiler from source \u00b6 Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host). Compile and run the model \u00b6 With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU. Compile the model \u00b6 IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then, Compile using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer. Run the model \u00b6 Run using the command-line \u00b6 In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"GPU - Vulkan"},{"location":"backends/gpu-vulkan/#vulkan-gpu-hal-driver","text":"IREE can accelerate model execution on GPUs via Vulkan , a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm. Todo Add IREE's GPU support matrix: what GPUs are supported; what GPUs are well optimized; etc.","title":"Vulkan GPU HAL Driver"},{"location":"backends/gpu-vulkan/#prerequisites","text":"In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps: Android Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher. Linux Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover. If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU. Windows Run the following command in a shell: vulkaninfo | grep apiVersion If vulkaninfo does not exist, you will need to install the latest Vulkan SDK . If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.","title":"Prerequisites"},{"location":"backends/gpu-vulkan/#get-runtime-and-compiler","text":"","title":"Get runtime and compiler"},{"location":"backends/gpu-vulkan/#get-iree-runtime-with-vulkan-hal-driver","text":"Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan.","title":"Get IREE runtime with Vulkan HAL driver"},{"location":"backends/gpu-vulkan/#build-runtime-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan to the IREE_HAL_DRIVERS_TO_BUILD CMake list variable when configuring (for target).","title":"Build runtime from source"},{"location":"backends/gpu-vulkan/#get-compiler-for-spir-v-exchange-format","text":"Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.","title":"Get compiler for SPIR-V exchange format"},{"location":"backends/gpu-vulkan/#download-as-python-package","text":"Python packages for various IREE functionalities are regularly published on IREE's GitHub Releases page. Right now these are just snapshots of the main development branch. You can install the Python package containing the SPIR-V compiler by python -m pip install iree-compiler-snapshot \\ -f https://github.com/google/iree/releases Tip iree-translate is installed as /path/to/python/site-packages/iree/tools/core/iree-translate . You can find out the full path to the site-packages directory via the python -m site command.","title":"Download as Python package"},{"location":"backends/gpu-vulkan/#build-compiler-from-source","text":"Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms. If you want to explicitly specify HAL drivers to support, you will need to add Vulkan-SPIRV to the IREE_TARGET_BACKENDS_TO_BUILD CMake list variable when configuring (for host).","title":"Build compiler from source"},{"location":"backends/gpu-vulkan/#compile-and-run-the-model","text":"With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU.","title":"Compile and run the model"},{"location":"backends/gpu-vulkan/#compile-the-model","text":"IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR ) expected by main IREE compilers first. Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer . Then,","title":"Compile the model"},{"location":"backends/gpu-vulkan/#compile-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vulkan-spirv \\ iree_input.mlir -o mobilenet-vulkan.vmfb Todo Choose the suitable target triple for the current GPU where iree_input.mlir is the model's initial MLIR representation generated by IREE's TensorFlow importer.","title":"Compile using the command-line"},{"location":"backends/gpu-vulkan/#run-the-model","text":"","title":"Run the model"},{"location":"backends/gpu-vulkan/#run-using-the-command-line","text":"In the build directory, run the following command: iree/tools/iree-run-module \\ --driver = vulkan \\ --module_file = mobilenet-vulkan.vmfb \\ --entry_function = predict \\ --function_inputs = \"1x224x224x3xf32=0\" The above assumes the exported function in the model is named as predict and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see iree-run-module --help for the format to specify concrete values.","title":"Run using the command-line"},{"location":"bindings/c-api/","text":"C API bindings \u00b6 Prerequisites \u00b6","title":"C API"},{"location":"bindings/c-api/#c-api-bindings","text":"","title":"C API bindings"},{"location":"bindings/c-api/#prerequisites","text":"","title":"Prerequisites"},{"location":"bindings/python/","text":"Python bindings \u00b6 IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends. Prerequisites \u00b6 To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py Installing IREE packages \u00b6 Prebuilt packages \u00b6 For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable. Building from source \u00b6 See Building Python bindings page for instructions for building from source. Using the Python bindings \u00b6 Troubleshooting \u00b6","title":"Python"},{"location":"bindings/python/#python-bindings","text":"IREE offers Python bindings split into several packages, covering different components: PIP package name Description iree-compiler-snapshot IREE's generic compiler tools and helpers iree-runtime-snapshot IREE's runtime, including CPU and GPU backends iree-tools-tf-snapshot Tools for importing from TensorFlow iree-tools-tflite-snapshot Tools for importing from TensorFlow Lite iree-tools-xla-snapshot Tools for importing from XLA Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends.","title":"Python bindings"},{"location":"bindings/python/#prerequisites","text":"To use IREE's Python bindings, you will first need to install Python 3 and pip , as needed. Tip We recommend using virtual environments to manage python packages, such as through venv ( about , tutorial ): Linux python -m venv .venv source .venv/bin/activate Windows python -m venv . venv . venv \\ Scripts \\ activate . bat When done, run deactivate . Next, install packages: python -m pip install --upgrade pip python -m pip install numpy absl-py","title":"Prerequisites"},{"location":"bindings/python/#installing-iree-packages","text":"","title":"Installing IREE packages"},{"location":"bindings/python/#prebuilt-packages","text":"For now, packages can be installed from our GitHub releases : Minimal To install just the core IREE packages: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ --find-links https://github.com/google/iree/releases All packages To install IREE packages with tools for all frontends: python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ iree-tools-tflite-snapshot \\ iree-tools-xla-snapshot \\ --find-links https://github.com/google/iree/releases Info We plan to publish packages on PyPI as they become more stable.","title":"Prebuilt packages"},{"location":"bindings/python/#building-from-source","text":"See Building Python bindings page for instructions for building from source.","title":"Building from source"},{"location":"bindings/python/#using-the-python-bindings","text":"","title":"Using the Python bindings"},{"location":"bindings/python/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"bindings/tensorflow-lite/","text":"TensorFlow Lite bindings \u00b6 Prerequisites \u00b6","title":"TensorFlow Lite"},{"location":"bindings/tensorflow-lite/#tensorflow-lite-bindings","text":"","title":"TensorFlow Lite bindings"},{"location":"bindings/tensorflow-lite/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/","text":"Building IREE from source \u00b6 Under construction.","title":"Building IREE from source"},{"location":"building-from-source/#building-iree-from-source","text":"Under construction.","title":"Building IREE from source"},{"location":"building-from-source/android/","text":"Android cross-compilation \u00b6 Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK Prerequisites \u00b6 Host environment setup \u00b6 You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps. Install Android NDK and ADB \u00b6 The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide . Configure and build \u00b6 Host configuration \u00b6 Build and install on your host machine: cmake -B ../iree-build/ -DCMAKE_INSTALL_PREFIX = ../iree-build/install . cmake --build ../iree-build/ --target install Target configuration \u00b6 Build the runtime using the Android NDK toolchain: Linux cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device. Running Android tests \u00b6 Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine. Running tools directly \u00b6 Invoke the host compiler tools produce input files: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmla \\ iree/tools/test/iree-run-module.mlir \\ -o /tmp/iree-run-module-vmla.vmfb Push the Android runtime tools to the device, along with any input files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/iree-run-module-vmla.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmla \\ -module_file = /data/local/tmp/iree-run-module-vmla.vmfb \\ -entry_function = abs \\ -function_inputs = \"i32=-5\"","title":"Android cross-compilation"},{"location":"building-from-source/android/#android-cross-compilation","text":"Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture): IREE's compiler is built on the host and is used there to generate modules for the target IREE's runtime is built on the host for the target. The runtime is then either pushed to the target to run natively or is bundled into an Android APK","title":"Android cross-compilation"},{"location":"building-from-source/android/#prerequisites","text":"","title":"Prerequisites"},{"location":"building-from-source/android/#host-environment-setup","text":"You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.","title":"Host environment setup"},{"location":"building-from-source/android/#install-android-ndk-and-adb","text":"The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here , or, if you have installed Android Studio , you can follow this guide instead. Note Make sure the ANDROID_NDK environment variable is set after installing the NDK. ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide .","title":"Install Android NDK and ADB"},{"location":"building-from-source/android/#configure-and-build","text":"","title":"Configure and build"},{"location":"building-from-source/android/#host-configuration","text":"Build and install on your host machine: cmake -B ../iree-build/ -DCMAKE_INSTALL_PREFIX = ../iree-build/install . cmake --build ../iree-build/ --target install","title":"Host configuration"},{"location":"building-from-source/android/#target-configuration","text":"Build the runtime using the Android NDK toolchain: Linux cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \" ${ ANDROID_NDK ? } /build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \" $PWD /../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Windows cmake -B ../iree-build-android/ \\ -DCMAKE_TOOLCHAIN_FILE = \"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\ -DIREE_HOST_BINARY_ROOT = \"%CD%/../iree-build/install\" \\ -DANDROID_ABI = \"arm64-v8a\" \\ -DANDROID_PLATFORM = \"android-29\" \\ -DIREE_BUILD_COMPILER = OFF \\ . cmake --build ../iree-build-android/ Note See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android. The specific ANDROID_ABI and ANDROID_PLATFORM used should match your target device.","title":"Target configuration"},{"location":"building-from-source/android/#running-android-tests","text":"Make sure you enable developer options and USB debugging on your Android device and can see your it when you run adb devices , then run all built tests through CTest : cd ../iree-build-android/ ctest --output-on-failure This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.","title":"Running Android tests"},{"location":"building-from-source/android/#running-tools-directly","text":"Invoke the host compiler tools produce input files: ../iree-build/install/bin/iree-translate \\ -iree-mlir-to-vm-bytecode-module \\ -iree-hal-target-backends = vmla \\ iree/tools/test/iree-run-module.mlir \\ -o /tmp/iree-run-module-vmla.vmfb Push the Android runtime tools to the device, along with any input files: adb push ../iree-build-android/iree/tools/iree-run-module /data/local/tmp/ adb shell chmod +x /data/local/tmp/iree-run-module adb push /tmp/iree-run-module-vmla.vmfb /data/local/tmp/ Run the tool: adb shell /data/local/tmp/iree-run-module -driver = vmla \\ -module_file = /data/local/tmp/iree-run-module-vmla.vmfb \\ -entry_function = abs \\ -function_inputs = \"i32=-5\"","title":"Running tools directly"},{"location":"building-from-source/getting-started/","text":"Getting started \u00b6 Prerequisites \u00b6 You will need to install CMake , along with a C/C++ compiler: Linux sudo apt install cmake clang export CC = clang export CXX = clang++ Windows Install CMake from the official downloads page Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details. Clone and build \u00b6 Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: cmake -B ../iree-build/ . cmake --build ../iree-build/ What's next? \u00b6 Running tests \u00b6 Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure Take a look around \u00b6 Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Getting started"},{"location":"building-from-source/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"building-from-source/getting-started/#prerequisites","text":"You will need to install CMake , along with a C/C++ compiler: Linux sudo apt install cmake clang export CC = clang export CXX = clang++ Windows Install CMake from the official downloads page Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the official downloads page Note You will need to initialize MSVC by running vcvarsall.bat to use it from the command line. See the official documentation for details.","title":"Prerequisites"},{"location":"building-from-source/getting-started/#clone-and-build","text":"Use Git to clone the IREE repository and initialize its submodules: git clone https://github.com/google/iree.git cd iree git submodule update --init Configure then build all targets using CMake: cmake -B ../iree-build/ . cmake --build ../iree-build/","title":"Clone and build"},{"location":"building-from-source/getting-started/#whats-next","text":"","title":"What's next?"},{"location":"building-from-source/getting-started/#running-tests","text":"Run all built tests through CTest : cd ../iree-build/ ctest --output-on-failure","title":"Running tests"},{"location":"building-from-source/getting-started/#take-a-look-around","text":"Check out the contents of the 'tools' build directory: ls ../iree-build/iree/tools/ ../iree-build/iree/tools/iree-translate --help","title":"Take a look around"},{"location":"building-from-source/optional-features/","text":"Optional Features \u00b6 This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page. Building Python Bindings \u00b6 This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Setup We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv .venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source .venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate . Usage From the iree-build directory: Linux and MacOS cmake -DIREE_BUILD_PYTHON_BINDINGS = ON -DPython3_EXECUTABLE = \" $( which python ) \" . ninja # Add ./bindings/python to PYTHONPATH and use the API. export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -DIREE_BUILD_PYTHON_BINDINGS = ON . ninja # Add bindings\\python to PYTHONPATH and use the API. set PYTHONPATH = \"$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest.","title":"Optional features"},{"location":"building-from-source/optional-features/#optional-features","text":"This page details the optional features and build modes for the project. Most of these are controlled by various CMake options, sometimes requiring extra setup or preparation. Each section extends the basic build steps in the getting started page.","title":"Optional Features"},{"location":"building-from-source/optional-features/#building-python-bindings","text":"This section describes how to build and interactively use built-from-source Python bindings for the following packages: Python Import Description import iree.compiler IREE's generic compiler tools and helpers import iree.runtime IREE's runtime, including CPU and GPU backends Also see instructions for installing pre-built binaries . Pre-requisites: A relatively recent Python3 installation (we aim to support non-eol Python versions ). Installation of python dependencies as specified in bindings/python/build_requirements.txt . CMake Variables: IREE_BUILD_PYTHON_BINDINGS : BOOL Enables building of Python bindings under bindings/python in the repository. Defaults to OFF . Python3_EXECUTABLE : PATH Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option. Setup We recommend using virtual environments to manage python packages, such as through venv , which may need to be installed via your system package manager ( about , tutorial ): Linux and MacOS # Make sure your 'python' is what you expect. Note that on multi-python # systems, this may have a version suffix, and on many Linuxes and MacOS where # python2 and python3 co-exist, you may also want to use `python3`. which python python --version # Create a persistent virtual environment (first time only). python -m venv .venv # Activate the virtual environment (per shell). # Now the `python` command will resolve to your virtual environment # (even on systems where you typically use `python3`). source .venv/bin/activate # Upgrade PIP. On Linux, many packages cannot be installed for older # PIP versions. See: https://github.com/pypa/manylinux python -m pip install --upgrade pip # Install IREE build pre-requisites. python -m pip install -r ./bindings/python/build_requirements.txt Windows python -m venv . venv . venv \\ Scripts \\ activate . bat python -m pip install - -upgrade pip python -m pip install -r bindings \\ python \\ build_requirements . txt When done, close your shell or run deactivate . Usage From the iree-build directory: Linux and MacOS cmake -DIREE_BUILD_PYTHON_BINDINGS = ON -DPython3_EXECUTABLE = \" $( which python ) \" . ninja # Add ./bindings/python to PYTHONPATH and use the API. export PYTHONPATH = \" $PWD /bindings/python\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Windows cmake -DIREE_BUILD_PYTHON_BINDINGS = ON . ninja # Add bindings\\python to PYTHONPATH and use the API. set PYTHONPATH = \"$pwd\\bindings\\python;%PYTHONPATH%\" python -c \"import iree.compiler\" python -c \"import iree.runtime\" Tests can now be run individually via python or via ctest.","title":"Building Python Bindings"},{"location":"community/projects/","text":"Community projects \u00b6 The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Projects"},{"location":"community/projects/#community-projects","text":"The IREE C++ Template demonstrates how to integrate IREE into a third-party project with CMake. The project demonstrates the usage of runtime support and how to use a custom dialect alongside with the runtime. The IREE LLVM Sandbox contains experimental work by the IREE team closely related to LLVM and MLIR, usually with the aim of contributing back to those upstream projects in some form.","title":"Community projects"},{"location":"frontends/jax/","text":"JAX frontend \u00b6 Prerequisites \u00b6","title":"JAX"},{"location":"frontends/jax/#jax-frontend","text":"","title":"JAX frontend"},{"location":"frontends/jax/#prerequisites","text":"","title":"Prerequisites"},{"location":"frontends/tensorflow-lite/","text":"TensorFlow Lite frontend \u00b6 Prerequisites \u00b6","title":"TensorFlow Lite"},{"location":"frontends/tensorflow-lite/#tensorflow-lite-frontend","text":"","title":"TensorFlow Lite frontend"},{"location":"frontends/tensorflow-lite/#prerequisites","text":"","title":"Prerequisites"},{"location":"frontends/tensorflow/","text":"TensorFlow frontend \u00b6 IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format . Prerequisites \u00b6 Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases Importing models \u00b6 IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-tf-import command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers. From SavedModel on TensorFlow Hub \u00b6 IREE supports importing and using SavedModels from TensorFlow Hub . Using the command-line tool \u00b6 First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( imported_with_signatures . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-tf-import . You can read the options supported via iree-tf-import -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-tf-import -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-tf-import is installed as /path/to/python/site-packages/iree/tools/tf/iree-tf-import . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU . Training \u00b6 Samples \u00b6 Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory. Troubleshooting \u00b6 Missing serving signature in SavedModel \u00b6 Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"TensorFlow"},{"location":"frontends/tensorflow/#tensorflow-frontend","text":"IREE supports compiling and running TensorFlow programs represented as tf.Module classes or stored in the SavedModel format .","title":"TensorFlow frontend"},{"location":"frontends/tensorflow/#prerequisites","text":"Install TensorFlow by following the official documentation : python -m pip install tf-nightly Install IREE pip packages, either from pip or by building from source : python -m pip install \\ iree-compiler-snapshot \\ iree-runtime-snapshot \\ iree-tools-tf-snapshot \\ -f https://github.com/google/iree/releases","title":"Prerequisites"},{"location":"frontends/tensorflow/#importing-models","text":"IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the iree-tf-import command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR ) compatible with the generic IREE compilers.","title":"Importing models"},{"location":"frontends/tensorflow/#from-savedmodel-on-tensorflow-hub","text":"IREE supports importing and using SavedModels from TensorFlow Hub .","title":"From SavedModel on TensorFlow Hub"},{"location":"frontends/tensorflow/#using-the-command-line-tool","text":"First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) print ( list ( imported_with_signatures . signatures . keys ())) Note If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\" . Then you can import the model with iree-tf-import . You can read the options supported via iree-tf-import -help . Using MobileNet v2 as an example and assuming the serving signature is predict : iree-tf-import -tf-import-type = savedmodel_v1 \\ -tf-savedmodel-exported-names = predict \\ /path/to/savedmodel -o iree_input.mlir Tip iree-tf-import is installed as /path/to/python/site-packages/iree/tools/tf/iree-tf-import . You can find out the full path to the site-packages directory via the python -m site command. -tf-import-type needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump. Afterwards you can further compile the model in iree_input.mlir for CPU or GPU .","title":"Using the command-line tool"},{"location":"frontends/tensorflow/#training","text":"","title":"Training"},{"location":"frontends/tensorflow/#samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.","title":"Samples"},{"location":"frontends/tensorflow/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"frontends/tensorflow/#missing-serving-signature-in-savedmodel","text":"Sometimes SavedModels are exported without explicit serving signatures . This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2 , assuming we want the serving signature to be predict and operating on a 224x224 RGB image: import tensorflow.compat.v2 as tf loaded_model = tf . saved_model . load ( '/path/to/downloaded/model/' ) call = loaded_model . __call__ . get_concrete_function ( tf . TensorSpec ([ 1 , 224 , 224 , 3 ], tf . float32 )) signatures = { 'predict' : call } tf . saved_model . save ( loaded_model , '/path/to/resaved/model/' , signatures = signatures ) The above will create a new SavedModel with a serving signature, predict , and save it to /path/to/resaved/model/ .","title":"Missing serving signature in SavedModel"}]}